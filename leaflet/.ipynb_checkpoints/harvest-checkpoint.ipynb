{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvest BTAA Geoportals\n",
    "\n",
    "> Original created on Dec 16 2020 <br>\n",
    "Edited on Jan 28 2021 -- Converted GeoJSON into TopoJSON to reduce file size<br>\n",
    "@author: Yijing Zhou @YijingZhou33 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import requests\n",
    "import geopandas as gpd\n",
    "import folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that **<a href='https://pypi.org/project/mapclassify/'>mapclassify</a>**, **<a href='https://seaborn.pydata.org/'>seaborn</a>** and **<a href='https://mattijn.github.io/topojson/'>topojson</a>** aren't built-in modules in Anaconda. You may need to install in advance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install mapclassify \n",
    "# pip install seaborn\n",
    "# pip install topojson\n",
    "import mapclassify \n",
    "import seaborn as sns \n",
    "import topojson as tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********** Input Files **********\n",
    "## Raw data: CSV files\n",
    "stategeoportals = os.path.join('data', 'allStates.csv')\n",
    "countygeoportals = os.path.join('data', 'allCounties.csv')\n",
    "citygeoportals = os.path.join('data', 'allCities.csv')\n",
    "\n",
    "## Basemap GeoJSON files for states and counties \n",
    "statejson = os.path.join('data', 'states.json')\n",
    "countyjson = os.path.join('data', 'counties.json')\n",
    "\n",
    "# ********** Output Files **********\n",
    "activestates = os.path.join('json', 'activeStates.topo.json')\n",
    "activecounties = os.path.join('json', 'activeCounties.topo.json')\n",
    "activecities = os.path.join('json', 'activeCities.json')\n",
    "\n",
    "legendjson = os.path.join('json', 'legend.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Part 1: State Geoportals TopoJSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format state name in state geoportals spreadsheet `allStates.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = pd.read_csv(stategeoportals)\n",
    "df_csv['btaaURL'] = df_csv['btaaURL'].apply(lambda x: x.split('-')[0])\n",
    "df_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etract total records number from BTAA Geoportal search page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def totalRecords(df):\n",
    "    totalrecords = []\n",
    "    for _, row in df.iterrows():\n",
    "        url = row['btaaURL']        \n",
    "        ## Start session and get the search page\n",
    "        session = requests.Session()\n",
    "        response = session.get(url)\n",
    "        ## Parse only part of the page (<meta> tag) for better performance using SoupStrainer and lxml\n",
    "        strainer = SoupStrainer('meta', attrs={'name': 'totalResults'})\n",
    "        soup = BeautifulSoup(response.content, 'lxml', parse_only=strainer)\n",
    "        ## The find() method looks through <meta> tagâ€™s descendants and retrieves one result with attribute 'name'\n",
    "        meta_tag = soup.find('meta', attrs={'name': 'totalResults'})\n",
    "        ## Grab the content inside the <meta> tag that matches the filter\n",
    "        totalrecord = meta_tag.get('content')\n",
    "        totalrecords.append(totalrecord)\n",
    "    return totalrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv['totalRecords'] = totalRecords(df_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the numinum number of total records\n",
    "If it equals to 0, meaning the landing page is 404 Not Found. Go back to check if the identifier is still active. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_totalRecords(df):\n",
    "    df['totalRecords'] = df['totalRecords'].astype(int)\n",
    "    if df['totalRecords'].min() == 0:\n",
    "        return df[df['totalRecords']==0]\n",
    "    else:\n",
    "        print('> State Geoportal Codes all valid!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_totalRecords(df_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group dataframe rows into list by geoportal sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_to_array(data):\n",
    "    groupItems = ['stateCode', 'Title', 'sourceURL']\n",
    "    for i in range(len(groupItems)):\n",
    "        data[groupItems[i]] = np.tile([data[groupItems[i]].values], (data.shape[0], 1)).tolist()\n",
    "    return data\n",
    "\n",
    "## Group by ['State']\n",
    "df_group = df_csv.groupby(['State']).apply(aggregate_to_array).drop_duplicates(subset=['State'])\n",
    "df_group.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge state GeoJSON and geoportal GeoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load statejson featuer properties\n",
    "state_geojson = gpd.read_file(statejson)\n",
    "state_json = json.loads(state_geojson.to_json())\n",
    "df_allState = pd.json_normalize(state_json['features'])\n",
    "\n",
    "## Change column names for further operation\n",
    "df_allState = df_allState[['properties.State', 'geometry.coordinates']].rename(\n",
    "    columns={'properties.State':'State', 'geometry.coordinates':'boundingBox'})\n",
    "\n",
    "df_allState.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Join on column 'State' from left dataframe df_group\n",
    "df_merge = pd.merge(df_group, df_allState, on = 'State', how = 'left')\n",
    "df_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create state GeoJSON features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_geojson_features(df):\n",
    "    print('> Creating state GeoJSON features...')\n",
    "    features = []\n",
    "    geometry_type = ''\n",
    "    geojson = {\n",
    "        'type': 'FeatureCollection',\n",
    "        'features': features\n",
    "    }\n",
    "        \n",
    "    for _, row in df.iterrows():\n",
    "        if type(row['boundingBox'][0][0][0]) is float:\n",
    "            geometry_type = 'Polygon'\n",
    "        else:\n",
    "            geometry_type = 'MultiPolygon'\n",
    "            \n",
    "        feature = {\n",
    "            'type': 'Feature',\n",
    "            'geometry': {\n",
    "                'type': geometry_type, \n",
    "                'coordinates': row['boundingBox']\n",
    "            },\n",
    "            'properties': {\n",
    "                'State': row['State'],\n",
    "                'Title': '|'.join([str(elem) for elem in row['Title']]),\n",
    "                'sourceURL': '|'.join([str(elem) for elem in row['sourceURL']]), \n",
    "                'btaaURL': row['btaaURL'],\n",
    "                'totalRecords': row['totalRecords']\n",
    "            }\n",
    "           }\n",
    "\n",
    "        features.append(feature)\n",
    "    return geojson\n",
    "\n",
    "data_geojson = create_geojson_features(df_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to state TopoJSON file `activeStates.topo.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_geojson = gpd.GeoDataFrame.from_features(data_geojson[\"features\"])\n",
    "# TopoJSON is an extension of GeoJSON to compress geometry information\n",
    "topo = tp.Topology(state_geojson)\n",
    "topo.to_json(activestates)\n",
    "print('> Creating state TopoJSON file...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect state TopoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topo.to_alt().properties(title='State Topology')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Part 2: County Geoportals TopoJSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format county name in county geoportals spreadsheet `allCounties.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = pd.read_csv(countygeoportals)\n",
    "\n",
    "## Replace 'Saint' and 'St' with 'St.'\n",
    "df_csv['County'] = df_csv['County'].apply(lambda x: re.sub(r'(Saint\\s|^St\\s|^St\\.\\s)', 'St. ', x))\n",
    "\n",
    "df_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etract total records number from BTAA Geoportal search page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv['totalRecords'] = totalRecords(df_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the numinum number of total records\n",
    "If it equals to 0, meaning the landing page is 404 Not Found. Go back to check if the identifier is still active. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_totalRecords(df_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group dataframe rows into list by geoportal sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_to_array(data):\n",
    "    groupItems = ['Title', 'sourceURL', 'totalRecords']\n",
    "    for i in range(len(groupItems)):\n",
    "        data[groupItems[i]] = np.tile([data[groupItems[i]].values], (data.shape[0], 1)).tolist()\n",
    "    return data\n",
    "\n",
    "## Group by ['County', 'State']\n",
    "df_group = df_csv.groupby(['County', 'State']).apply(aggregate_to_array).drop_duplicates(subset=['County', 'State'])\n",
    "## Sum up the total records if there're multiple geoportals in one county\n",
    "df_group['totalRecords'] = df_group['totalRecords'].apply(lambda x: sum(int(item)for item in x))\n",
    "df_group.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify the geoportal by total number\n",
    "You may want to adjust the classification method **`NaturalBreaks`** and class number **`k`**. \n",
    "Reference the <a href=\"https://pypi.org/project/mapclassify/\">mapclassify</a>. Note that we're going to exclude the outler `totaLRecords == 1` first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excludeOne = df_group[df_group['totalRecords'] != 1]\n",
    "n4 = mapclassify.NaturalBreaks(df_excludeOne.totalRecords, k=4)\n",
    "n4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countyInterval = [1.0] + [i for i in list(n4.bins)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign different color to each geoportal based on total records class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select the gradient color palette\n",
    "palette = ['#b3cde0','#6497b1','#005b96','#03396c ','#011f4b']\n",
    "colorScale = np.array(['#b3cde0','#6497b1','#005b96','#03396c','#011f4b'])\n",
    "sns.palplot(sns.color_palette(colorScale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def totalRecords_color(row):\n",
    "    if row['totalRecords'] <= countyInterval[0]:\n",
    "        return palette[0]\n",
    "    elif row['totalRecords'] > countyInterval[0] and row['totalRecords'] <= countyInterval[1]:\n",
    "        return palette[1]\n",
    "    elif row['totalRecords'] > countyInterval[1] and row['totalRecords'] <= countyInterval[2]:\n",
    "        return palette[2]\n",
    "    elif row['totalRecords'] > countyInterval[2] and row['totalRecords'] <= countyInterval[3]:\n",
    "        return palette[3]\n",
    "    else:\n",
    "        return palette[4]\n",
    "\n",
    "## Append a new column with color generated above\n",
    "df_group['Color'] = df_group.apply(totalRecords_color, axis=1)\n",
    "df_group.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge county GeoJSON and geoportal GeoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load countyjson featuer properties\n",
    "county_geojson = gpd.read_file(countyjson)\n",
    "county_json = json.loads(county_geojson.to_json())\n",
    "df_allCounty = pd.json_normalize(county_json['features'])\n",
    "\n",
    "## Change column names for further operation\n",
    "df_allCounty = df_allCounty[['properties.County', 'properties.State', 'geometry.coordinates']].rename(\n",
    "    columns={'properties.County':'County', 'properties.State':'State', 'geometry.coordinates':'boundingBox'})\n",
    "\n",
    "df_allCounty.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Join on column 'County' and 'State' from left dataframe df_group\n",
    "df_merge = pd.merge(df_group, df_allCounty, on = ['County','State'], how = 'left')\n",
    "df_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return rows with Nan value\n",
    "Check if there exists any records doesn't include any coordinates information in the boundingBox column. <br>\n",
    "If so, go back to `allCounties.csv` and manually change the **county** name to the one in `county.json`, then go to `Kernel` > `Restart & Run All`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nanrows(df):\n",
    "    if df.isnull().values.any():\n",
    "        return df[df['boundingBox'].isnull()]\n",
    "    else:\n",
    "        print('> No NULL rows')        \n",
    "check_nanrows(df_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create county GeoJSON features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_geojson_features(df):\n",
    "    print('> Creating county GeoJSON features...')\n",
    "    features = []\n",
    "    geometry_type = ''\n",
    "    geojson = {\n",
    "        'type': 'FeatureCollection',\n",
    "        'features': features\n",
    "    }\n",
    "        \n",
    "    for _, row in df.iterrows():\n",
    "        if type(row['boundingBox'][0][0][0]) is float:\n",
    "            geometry_type = 'Polygon'\n",
    "        else:\n",
    "            geometry_type = 'MultiPolygon'\n",
    "            \n",
    "        feature = {\n",
    "            'type': 'Feature',\n",
    "            'geometry': {\n",
    "                'type': geometry_type, \n",
    "                'coordinates': row['boundingBox']\n",
    "            },\n",
    "            'properties': {\n",
    "                'County': row['County'],\n",
    "                'State': row['State'],\n",
    "                'countyCode': row['countyCode'],\n",
    "                'Title': '|'.join([str(elem) for elem in row['Title']]),\n",
    "                'sourceURL': '|'.join([str(elem) for elem in row['sourceURL']]), \n",
    "                'btaaURL': row['btaaURL'],\n",
    "                'totalRecords': row['totalRecords'],\n",
    "                'Color' : row['Color']\n",
    "            }\n",
    "           }\n",
    "\n",
    "        features.append(feature)\n",
    "    return geojson\n",
    "\n",
    "data_geojson = create_geojson_features(df_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to county TopoJSON file `activeCounties.topo.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "county_geojson = gpd.GeoDataFrame.from_features(data_geojson[\"features\"])\n",
    "# TopoJSON is an extension of GeoJSON to compress geometry information\n",
    "topo = tp.Topology(county_geojson)\n",
    "topo.to_json(activecounties)\n",
    "print('> Creating county TopoJSON file...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect county TopoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topo.to_alt().properties(title='County Topology')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Part 3: City Geoportals GeoJSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format city name in city geoportals spreadsheet `allCities.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(citygeoportals)\n",
    "## Calculate city coordinates and round to 2 decimal places \n",
    "df = pd.concat([df, df['Bounding Box'].str.split(',', expand=True).astype(float)], axis=1).rename(\n",
    "    columns={0:'minX', 1:'minY', 2:'maxX', 3:'maxY'})\n",
    "df['centerX'] = round((df['minX'] + df['maxX']) / 2, 2)\n",
    "df['centerY'] = round((df['minY'] + df['maxY']) / 2, 2)\n",
    "df_clean = df.drop(columns =['minX', 'minY', 'maxX', 'maxY', 'Bounding Box'])\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etract total records number from BTAA Geoportal search page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['totalRecords'] = totalRecords(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the numinum number of total records\n",
    "If it equals to 0, meaning the landing page is 404 Not Found. Go back to check if the identifier is still active. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_totalRecords(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group dataframe rows into list by geoportal sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_to_array(data):\n",
    "    groupItems = ['Title', 'sourceURL', 'totalRecords']\n",
    "    for i in range(len(groupItems)):\n",
    "        data[groupItems[i]] = np.tile([data[groupItems[i]].values], (data.shape[0], 1)).tolist()\n",
    "    return data\n",
    "\n",
    "## Group by ['City', 'State']\n",
    "df_group = df_clean.groupby(['centerX']).apply(aggregate_to_array).drop_duplicates(subset=['City', 'State'])\n",
    "# sum up the total records if there're multiple geoportals in one city\n",
    "df_group['totalRecords'] = df_group['totalRecords'].apply(lambda x: sum(int(item)for item in x))\n",
    "df_group.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify the geoportal by total number\n",
    "You may want to adjust the classification method **`Quantiles`** and class number **`k`**. \n",
    "Reference the <a href=\"https://pypi.org/project/mapclassify/\">mapclassify</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n3 = mapclassify.Quantiles(df_group.totalRecords, k=3)\n",
    "n3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityInterval = list(n3.bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign different circle radius to each geoportal based on total records class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Size of symbols on map in meters \n",
    "size = [12000, 16000, 22000]\n",
    "## Size of symbols inside legend in pixels\n",
    "legendSize = [12, 18, 28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def totalRecords_size(row):\n",
    "    if row['totalRecords'] <= cityInterval[0]:\n",
    "        return size[0]\n",
    "    elif row['totalRecords'] > cityInterval[0] and row['totalRecords'] <= cityInterval[1]:\n",
    "        return size[1]\n",
    "    else:\n",
    "        return size[2]\n",
    "\n",
    "## Append a new column with color generated above\n",
    "df_group['Size'] = df_group.apply(totalRecords_size, axis=1)\n",
    "df_group.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create city GeoJSON features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_geojson_features(df):\n",
    "    print('> Creating city GeoJSON features...')\n",
    "    features = []\n",
    "    geojson = {\n",
    "        'type': 'FeatureCollection',\n",
    "        'features': features\n",
    "    }\n",
    "    for _, row in df.iterrows():\n",
    "        feature = {\n",
    "            'type': 'Feature',\n",
    "            'geometry': {\n",
    "                'type':'Point', \n",
    "                'coordinates':[row['centerX'], row['centerY']]\n",
    "            },\n",
    "            'properties': {\n",
    "                'City': row['City'],\n",
    "                'State': row['State'],\n",
    "                'Title': '|'.join([str(elem) for elem in row['Title']]),\n",
    "                'sourceURL': '|'.join([str(elem) for elem in row['sourceURL']]), \n",
    "                'btaaURL': row['btaaURL'],\n",
    "                'totalRecords': row['totalRecords'],\n",
    "                'Size' : row['Size']\n",
    "            }\n",
    "           }\n",
    "\n",
    "        features.append(feature)\n",
    "    return geojson\n",
    "\n",
    "data_geojson = create_geojson_features(df_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to city GeoJSON file `activecities.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(activecities, 'w') as txtfile:\n",
    "    json.dump(data_geojson, txtfile)\n",
    "print('> Creating city GeoJSON file...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect city GeoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('> Making map...')\n",
    "m = folium.Map(location = [44, -90], control_scale = True, zoom_start = 6)\n",
    "\n",
    "lyrCity = folium.GeoJson(open(activecities, 'r').read(),\n",
    "               tooltip = folium.GeoJsonTooltip(fields=('City', 'sourceURL'),\n",
    "                                               aliases=('city','websiteUrl')),\n",
    "               show = True).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Part 4: Legend JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create legend JSON features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_legend_json(countyinterval, palette, cityinterval, size):\n",
    "    print('> Creating legend JSON featuers...')\n",
    "    countystyle = dict(zip(countyinterval, palette))\n",
    "    citystyle = dict(zip(cityinterval, size))\n",
    "    dic = {\n",
    "            'county':countystyle,\n",
    "              'city':citystyle\n",
    "          }\n",
    "    return dic\n",
    "    \n",
    "data_json = create_legend_json(countyInterval, palette, cityInterval, legendSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to legend JSON file `legend.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(legendjson, 'w') as txtfile:\n",
    "    json.dump(data_json, txtfile)\n",
    "print('> Creating legend JSON file...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
